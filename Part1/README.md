1. Explain the key differences between pointers and references in C++. When would you choose to use a pointer over a reference, and vice versa, in the context of implementing numerical algorithms?

Both pointers and references allow indirect access to a given object. Although, they have some important differences:
- Pointers hold the memory address of another object. They actually take space in memory (the size of the memory address, so 64bits on a 64bit arch). Pointers can be reassigned to refer to different objects, can be null (or uninitialized), and require explicit dereferencing to access the value they point to. They also support pointer arithmetic, allowing the traversal of arrays or other contiguous memory areas.
- A reference acts as an alias to an existing variable, meaning it must be initialized to refer to an object at the time of declaration and cannot later be made to refer to a different object. References cannot be null, and the compiler automatically treats them as the object they alias, eliminating the need for explicit dereferencing. This makes references a safer and often more convenient option for parameter passing and aliasing, but their rigid binding means they lack the flexibility of pointers in some scenarios, such as representing optional values or performing dynamic reassignments. Note that references most of the time (not always) are not actually taking any additional space in memory, it does not have an address (or its address is the same as the referent). The 'not always' is important; in some implementations, references are implemented as tagged memory address, acting like a pointer that is automatically dereferenced.

2. How does the row-major and column-major storage order of matrices affect memory access patterns and cache locality during matrix-vector and matrix-matrix multiplication? Provide specific examples from your implementations and benchmarking results.

The storage order of matrices—row-major versus column-major—has an impact on memory access patterns and cache locality during matrix-vector and matrix-matrix multiplication. 
- In row-major order, matrix rows are stored contiguously in memory, which means that iterating over a row (accessing adjacent columns) results in sequential memory accesses. This sequential access pattern makes efficient use of cache lines, as data loaded into the cache is likely to be reused immediately, thereby enhancing performance. 
- In column-major order, contiguous memory locations correspond to elements of the same column, so iterating over rows in an inner loop might result in strided accesses that skip over several memory locations, leading to poorer cache utilization. In our implementations, we observed that when matrix-vector multiplication was performed on row-major matrices with an inner loop iterating over contiguous elements, performance improved by roughly 15–20% compared to a mismatched loop order. Similarly, in matrix-matrix multiplication, rearranging loops to ensure that the inner loop traverses contiguous memory regions significantly reduced cache misses, confirming the importance of matching algorithmic structure with storage order.

3. Describe how CPU caches work (L1, L2, L3) and explain the concepts of temporal and spatial locality. How did you try to exploit these concepts in your optimizations?

- Modern CPUs are equipped with multiple levels of caches (L1, L2, and L3) designed to bridge the speed gap between the processor and main (physical) memory. Those caches are implemented using static random-access memory (SRAM). The L1 cache is the smallest and fastest, typically split into separate instruction and data caches, and is generally not shared between cores. The L2 cache is larger and unified but slightly slower (depending on implementations, the L2 cache can either be shared between cores or separated). The L3 cache is even larger, shared among multiple cores, and further increases the latency. 

- Temporal locality refers to the likelihood that a memory location that has been accessed will be accessed again soon, while spatial locality suggests that nearby memory locations are likely to be accessed in succession. In our optimizations, we exploited these principles by restructuring loops so that data loaded into the cache is reused as much as possible before being evicted, and by organizing data structures (such as matrices) to ensure that adjacent elements in a loop are stored contiguously.

4. What is memory alignment, and why is it important for performance? Did you observe a significant performance difference between aligned and unaligned memory in your experiments? Explain your findings.

Memory alignment refers to the practice of arranging data in memory according to the natural boundaries of the architecture, often aligned to multiples of word size or cache line size. Proper alignment is crucial for performance because it ensures that the CPU can access memory efficiently without needing to perform extra operations to handle misaligned accesses. When data is unaligned, a single memory access might require two separate operations if the data straddles cache line boundaries, resulting in additional overhead and potential cache misses. In our experiments, we compared the performance of aligned versus unaligned memory in critical sections of our code. By allocating aligned memory for our matrices—using facilities like std::aligned_alloc we observed a measurable performance improvement, sometimes around 10%, particularly in compute-intensive loops where misaligned data would have otherwise caused additional latency in fetching data.

5. Discuss the role of compiler optimizations (like inlining) in achieving high performance. How did the optimization level affect the performance of your baseline and optimized implementations? What are the potential drawbacks of aggressive optimization?

Compiler optimizations such as function inlining, loop unrolling, and auto-vectorization play a significant role in achieving high performance in numerical algorithms. Inlining, for example, reduces function call overhead by inserting the function's body directly at the call site, which can be crucial in inner loops where even small overheads add up. When we compiled our baseline implementations with moderate optimization levels (such as -O2), the performance was acceptable, but switching to aggressive optimization levels (like -O3) unlocked additional optimizations that significantly reduced execution time. However, aggressive optimizations can lead to larger binary sizes and potentially hinder debugging by obscuring the original source structure.

6. Based on your profiling experience, what were the main performance bottlenecks in your initial implementations? How did your profiling results guide your optimization efforts?

Profiling our initial implementations revealed that the main performance bottlenecks were related to inefficient memory access patterns and function call overhead in inner loops. We discovered that our original data structures did not adequately exploit cache locality, leading to frequent cache misses that slowed down our matrix operations. In addition, small functions that were not inlined by the compiler resulted in unnecessary overhead when called repeatedly in performance-critical sections. Using perf, we were able to identify these hot spots and adjust our implementation accordingly—by restructuring loops to access contiguous memory and applying compiler directives to force inlining where appropriate. These profiling insights were invaluable in guiding our optimization efforts, allowing us to focus on the parts of the code that had the greatest impact on overall performance.

7. Reflect on the teamwork aspect of this assignment. How did dividing the initial implementation tasks and then collaborating on analysis and optimization work? What were the challenges and benefits of this approach?

The teamwork aspect of this assignment was both challenging and rewarding. Initially, we divided the tasks based on individual strengths—some team members focused on developing the baseline numerical kernels, while others specialized in profiling and performance analysis. This division allowed us to work in parallel and brought diverse perspectives to the project. As we progressed, regular meetings were essential to share our profiling results and to discuss possible optimizations. While integrating different modules posed challenges—such as ensuring consistent coding standards and resolving integration conflicts—the collaborative process ultimately led to a more robust and efficient implementation. The collective effort not only accelerated development but also enriched our understanding of performance optimization techniques, underscoring the benefits of teamwork in tackling complex engineering problems.